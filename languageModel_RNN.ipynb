{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We read the text(pg2265.txt) to remove the beginning\n",
    "# portion(that contains legal description), then construct \n",
    "# the dictionary based on the text\n",
    "\n",
    "# Processing the document\n",
    "with open('pg2265.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "# Excluding the legal portion    \n",
    "text = text[15858:]\n",
    "\n",
    "# Getting all the unique characters in the text\n",
    "chars = set(text)\n",
    "\n",
    "# Dictionary that maps each character to a integer\n",
    "char2int = {ch:i for i, ch in enumerate(chars)}\n",
    "\n",
    "# Dictionary that maps integers to characters\n",
    "int2char = dict(enumerate(chars))\n",
    "\n",
    "text_ints = np.array([char2int[ch] for ch in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{';': 0, '!': 1, 'R': 2, '[': 3, 'w': 4, 't': 5, \"'\": 6, 'k': 7, 'G': 8, 'K': 9, ']': 10, 'm': 11, 'D': 12, 'o': 13, 'x': 14, 'F': 15, 'p': 16, ' ': 17, 'B': 18, 'c': 19, 'i': 20, '(': 21, '1': 22, 'E': 23, 'd': 24, 'b': 25, 'L': 26, 's': 27, 'P': 28, '-': 29, ')': 30, '&': 31, 'v': 32, 'O': 33, 'f': 34, 'C': 35, 'H': 36, 'a': 37, 'Y': 38, 'Z': 39, 'I': 40, 'T': 41, 'j': 42, '.': 43, 'g': 44, 'A': 45, ',': 46, '\\n': 47, 'y': 48, ':': 49, 'u': 50, 'W': 51, 'M': 52, 'h': 53, 'q': 54, 'z': 55, 'e': 56, 'r': 57, 'V': 58, 'S': 59, '?': 60, 'Q': 61, 'n': 62, 'l': 63, 'N': 64}\n"
     ]
    }
   ],
   "source": [
    "print(char2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ';', 1: '!', 2: 'R', 3: '[', 4: 'w', 5: 't', 6: \"'\", 7: 'k', 8: 'G', 9: 'K', 10: ']', 11: 'm', 12: 'D', 13: 'o', 14: 'x', 15: 'F', 16: 'p', 17: ' ', 18: 'B', 19: 'c', 20: 'i', 21: '(', 22: '1', 23: 'E', 24: 'd', 25: 'b', 26: 'L', 27: 's', 28: 'P', 29: '-', 30: ')', 31: '&', 32: 'v', 33: 'O', 34: 'f', 35: 'C', 36: 'H', 37: 'a', 38: 'Y', 39: 'Z', 40: 'I', 41: 'T', 42: 'j', 43: '.', 44: 'g', 45: 'A', 46: ',', 47: '\\n', 48: 'y', 49: ':', 50: 'u', 51: 'W', 52: 'M', 53: 'h', 54: 'q', 55: 'z', 56: 'e', 57: 'r', 58: 'V', 59: 'S', 60: '?', 61: 'Q', 62: 'n', 63: 'l', 64: 'N'}\n"
     ]
    }
   ],
   "source": [
    "print(int2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47 41 53 ..., 56 43 47]\n"
     ]
    }
   ],
   "source": [
    "print(text_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reshaping the data into batches of sequences\n",
    "def reshape_data(sequence, batch_size, num_steps):\n",
    "    \n",
    "    batch_length = batch_size * num_steps\n",
    "    num_batches = int(len(sequence) / batch_length)\n",
    "    \n",
    "    if num_batches*batch_length + 1 > len(sequence):\n",
    "        num_batches = num_batches - 1\n",
    "        \n",
    "    # Truncate the sequence at the end to get rid of \n",
    "    # remaining charcaters that do not make a full batch\n",
    "    x = sequence[0 : num_batches*batch_length]\n",
    "    y = sequence[1 : num_batches*batch_length + 1]\n",
    "    \n",
    "    # Split x & y into a list batches of sequences: \n",
    "    x_batch_splits = np.split(x, batch_size)\n",
    "    y_batch_splits = np.split(y, batch_size)\n",
    "    \n",
    "    # Stack the batches together: batch_size*batch_length\n",
    "    x = np.stack(x_batch_splits)\n",
    "    y = np.stack(y_batch_splits)\n",
    "    \n",
    "    return(x,y)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting x & y into mini-batches where each row is\n",
    "# a sequence with length = num_steps\n",
    "\n",
    "def create_batch_generator(data_x, data_y, num_steps):\n",
    "    batch_size, batch_length = data_x.shape\n",
    "    \n",
    "    num_batches = int(batch_length/num_steps)\n",
    "    \n",
    "    for b in range(num_batches):\n",
    "        yield(data_x[:, b*num_steps:(b+1)*num_steps],\n",
    "              data_y[:, b*num_steps:(b+1)*num_steps])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defining RNN class \n",
    "\n",
    "class CharacterRNN(object):\n",
    "    \n",
    "    # boolean sampling decides whether instance is for building \n",
    "    # the graph in training mode or sampling mode\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, \n",
    "                 num_steps=100, lstm_size=128, num_layers=1,\n",
    "                 learning_rate=0.001, keep_prob=0.5, \n",
    "                 grad_clip=5, sampling=False):\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.keep_prob = keep_prob\n",
    "        self.grad_clip = grad_clip\n",
    "        \n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "            \n",
    "            self.build(sampling=sampling)\n",
    "            \n",
    "            self.saver = tf.train.Saver()\n",
    "            \n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "        \n",
    "        \n",
    "    # Builds the computation graph for our RNN\n",
    "    def build(self, sampling):\n",
    "        if(sampling==True):\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "            num_steps = self.num_steps\n",
    "\n",
    "        # Defining the input placeholders        \n",
    "        tf_x = tf.placeholder(tf.int32, \n",
    "                              shape=[batch_size, num_steps],\n",
    "                              name='tf_x')\n",
    "\n",
    "        tf_y = tf.placeholder(tf.int32, \n",
    "                              shape=[batch_size, num_steps],\n",
    "                              name='tf_y')\n",
    "\n",
    "        tf_keepprob = tf.placeholder(tf.float32, \n",
    "                                     name='tf_keepprob')\n",
    "\n",
    "        # One-hot encoding\n",
    "        x_onehot = tf.one_hot(tf_x, depth=self.num_classes)\n",
    "        y_onehot = tf.one_hot(tf_y, depth=self.num_classes)\n",
    "        \n",
    "        # Building the Multi-Layered RNN cell\n",
    "        cells = tf.contrib.rnn.MultiRNNCell(\n",
    "                    [tf.contrib.rnn.DropoutWrapper(\n",
    "                        tf.contrib.rnn.BasicLSTMCell(self.lstm_size), \n",
    "                        output_keep_prob=tf_keepprob)\n",
    "                     for _ in range(self.num_layers)])\n",
    "        \n",
    "        # Defining the initial state(zeros with required dimensions)\n",
    "        self.initial_state = cells.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        # Run each sequence step through the RNN\n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(cells, x_onehot,\n",
    "                                                           initial_state=self.initial_state)\n",
    "        \n",
    "        print(' <<< lstm_outputs >>>', lstm_outputs)\n",
    "        \n",
    "        seq_output_reshaped = tf.reshape(lstm_outputs, shape=[-1, self.lstm_size],\n",
    "                                         name='seq_output_reshaped')\n",
    "        \n",
    "        logits = tf.layers.dense(inputs=seq_output_reshaped,\n",
    "                                 units=self.num_classes,\n",
    "                                 activation=None,\n",
    "                                 name='logits')\n",
    "        \n",
    "        \n",
    "        proba = tf.nn.softmax(logits, name='probabilities')\n",
    "        print(proba)\n",
    "        \n",
    "        y_reshaped = tf.reshape(y_onehot, shape=[-1, self.num_classes],\n",
    "                                name='y_reshaped')\n",
    "        \n",
    "        cost = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(logits=logits, \n",
    "                                                            labels=y_reshaped),\n",
    "                    name='cost')\n",
    "        \n",
    "        # Gradient Clipping to avoid exploding gradients\n",
    "        tvars = tf.trainable_variables()\n",
    "        \n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), \n",
    "                                          self.grad_clip)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        \n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars), name='train_op')\n",
    "        \n",
    "    # Training our RNN    \n",
    "    def train(self, train_x, train_y,\n",
    "              num_epochs, ckpt_dir='./model/'):\n",
    "        \n",
    "        # Creating the check-point directory\n",
    "        if not os.path.exists(ckpt_dir):\n",
    "            os.mkdir(ckpt_dir)\n",
    "            \n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "            \n",
    "            n_batches = int(train_x.shape[1] / self.num_steps)\n",
    "            iterations = n_batches*num_epochs\n",
    "            \n",
    "            for epoch in range(num_epochs):\n",
    "                \n",
    "                # Train Network\n",
    "                new_state = sess.run(self.initial_state)\n",
    "                loss = 0\n",
    "                \n",
    "                # Generates Batches\n",
    "                bGen = create_batch_generator(train_x, train_y, self.num_steps)\n",
    "                \n",
    "                for b, (batch_x, batch_y) in enumerate(bGen, 1):\n",
    "                    iteration = epoch*n_batches + b\n",
    "        \n",
    "                    feed = {\n",
    "                            'tf_x:0' : batch_x,\n",
    "                            'tf_y:0' : batch_y,\n",
    "                            'tf_keepprob:0' : self.keep_prob,\n",
    "                            self.initial_state : new_state\n",
    "                    }\n",
    "            \n",
    "                    batch_cost, _, new_state = sess.run(['cost:0', 'train_op', \n",
    "                                                         self.final_state], feed_dict = feed)\n",
    "                \n",
    "                    if iteration % 10 == 0:\n",
    "                        \n",
    "                        print('Epoch %d/%d Iteration %d'\n",
    "                              '| Training loss: %.4f' % (epoch+1, num_epochs, iteration,\n",
    "                                                       batch_cost))\n",
    "                        \n",
    "                # Saving the trained model\n",
    "                self.saver.save(sess, os.path.join(ckpt_dir, 'language_modelling.ckpt'))\n",
    "    \n",
    "    # Predictions for RNN\n",
    "    def sample(self, output_length, \n",
    "               ckpt_dir, starter_seq=\"The \"):\n",
    "        \n",
    "        observed_seq = [ch for ch in starter_seq]    \n",
    "        \n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            self.saver.restore(\n",
    "                sess, \n",
    "                tf.train.latest_checkpoint(ckpt_dir))\n",
    "            \n",
    "            # Running the model using the starter sequence\n",
    "            new_state = sess.run(self.initial_state)\n",
    "            for ch in starter_seq:\n",
    "                \n",
    "                x = np.zeros((1, 1))\n",
    "                x[0,0] = char2int[ch]\n",
    "                feed = {'tf_x:0': x,\n",
    "                        'tf_keepprob:0': 1.0,\n",
    "                        self.initial_state: new_state}\n",
    "                \n",
    "                proba, new_state = sess.run(\n",
    "                        ['probabilities:0', self.final_state], \n",
    "                        feed_dict=feed)\n",
    "\n",
    "            ch_id = get_top_char(proba, len(chars))\n",
    "            observed_seq.append(int2char[ch_id])\n",
    "            \n",
    "            # Running the model using the updated observed_seq\n",
    "            for i in range(output_length):\n",
    "                \n",
    "                x[0,0] = ch_id\n",
    "                feed = {'tf_x:0': x,\n",
    "                        'tf_keepprob:0': 1.0,\n",
    "                        self.initial_state: new_state}\n",
    "                \n",
    "                proba, new_state = sess.run(\n",
    "                        ['probabilities:0', self.final_state], \n",
    "                        feed_dict=feed)\n",
    "\n",
    "                ch_id = get_top_char(proba, len(chars))\n",
    "                observed_seq.append(int2char[ch_id])\n",
    "\n",
    "        return ''.join(observed_seq) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Method to return to characters    \n",
    "def get_top_char(probas, char_size, top_n=5):\n",
    "    p = np.squeeze(probas)\n",
    "\n",
    "    # Sort the characters in descending order\n",
    "    p[np.argsort(p)[:-top_n]] = 0.0\n",
    "\n",
    "    p = p/np.sum(p)\n",
    "\n",
    "    # Return a randomly chosen character from top ones\n",
    "    char_id = np.random.choice(char_size, 1, p=p)[0]\n",
    "\n",
    "    return(char_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <<< lstm_outputs >>> Tensor(\"rnn/transpose_1:0\", shape=(64, 100, 128), dtype=float32)\n",
      "Tensor(\"probabilities:0\", shape=(6400, 65), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-8-68fe71b71d1c>:90: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Epoch 1/20 Iteration 10| Training loss: 3.6465\n",
      "Epoch 1/20 Iteration 20| Training loss: 3.3772\n",
      "Epoch 2/20 Iteration 30| Training loss: 3.2907\n",
      "Epoch 2/20 Iteration 40| Training loss: 3.2471\n",
      "Epoch 2/20 Iteration 50| Training loss: 3.2281\n",
      "Epoch 3/20 Iteration 60| Training loss: 3.2198\n",
      "Epoch 3/20 Iteration 70| Training loss: 3.1951\n",
      "Epoch 4/20 Iteration 80| Training loss: 3.1769\n",
      "Epoch 4/20 Iteration 90| Training loss: 3.1508\n",
      "Epoch 4/20 Iteration 100| Training loss: 3.1378\n",
      "Epoch 5/20 Iteration 110| Training loss: 3.1304\n",
      "Epoch 5/20 Iteration 120| Training loss: 3.1029\n",
      "Epoch 6/20 Iteration 130| Training loss: 3.0806\n",
      "Epoch 6/20 Iteration 140| Training loss: 3.0533\n",
      "Epoch 6/20 Iteration 150| Training loss: 3.0297\n",
      "Epoch 7/20 Iteration 160| Training loss: 2.9916\n",
      "Epoch 7/20 Iteration 170| Training loss: 2.9444\n",
      "Epoch 8/20 Iteration 180| Training loss: 2.8922\n",
      "Epoch 8/20 Iteration 190| Training loss: 2.8499\n",
      "Epoch 8/20 Iteration 200| Training loss: 2.8102\n",
      "Epoch 9/20 Iteration 210| Training loss: 2.7709\n",
      "Epoch 9/20 Iteration 220| Training loss: 2.7205\n",
      "Epoch 10/20 Iteration 230| Training loss: 2.6835\n",
      "Epoch 10/20 Iteration 240| Training loss: 2.6670\n",
      "Epoch 10/20 Iteration 250| Training loss: 2.6185\n",
      "Epoch 11/20 Iteration 260| Training loss: 2.6261\n",
      "Epoch 11/20 Iteration 270| Training loss: 2.5708\n",
      "Epoch 12/20 Iteration 280| Training loss: 2.5409\n",
      "Epoch 12/20 Iteration 290| Training loss: 2.5521\n",
      "Epoch 12/20 Iteration 300| Training loss: 2.5019\n",
      "Epoch 13/20 Iteration 310| Training loss: 2.5203\n",
      "Epoch 13/20 Iteration 320| Training loss: 2.4923\n",
      "Epoch 14/20 Iteration 330| Training loss: 2.4606\n",
      "Epoch 14/20 Iteration 340| Training loss: 2.4849\n",
      "Epoch 14/20 Iteration 350| Training loss: 2.4295\n",
      "Epoch 15/20 Iteration 360| Training loss: 2.4496\n",
      "Epoch 15/20 Iteration 370| Training loss: 2.4096\n",
      "Epoch 16/20 Iteration 380| Training loss: 2.3934\n",
      "Epoch 16/20 Iteration 390| Training loss: 2.4166\n",
      "Epoch 16/20 Iteration 400| Training loss: 2.3562\n",
      "Epoch 17/20 Iteration 410| Training loss: 2.3959\n",
      "Epoch 17/20 Iteration 420| Training loss: 2.3648\n",
      "Epoch 18/20 Iteration 430| Training loss: 2.3585\n",
      "Epoch 18/20 Iteration 440| Training loss: 2.3867\n",
      "Epoch 18/20 Iteration 450| Training loss: 2.3195\n",
      "Epoch 19/20 Iteration 460| Training loss: 2.3573\n",
      "Epoch 19/20 Iteration 470| Training loss: 2.3268\n",
      "Epoch 20/20 Iteration 480| Training loss: 2.3257\n",
      "Epoch 20/20 Iteration 490| Training loss: 2.3423\n",
      "Epoch 20/20 Iteration 500| Training loss: 2.2888\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_steps = 100\n",
    "\n",
    "train_x, train_y = reshape_data(text_ints, \n",
    "                                batch_size, \n",
    "                                num_steps)\n",
    "\n",
    "# Creating RNN onject\n",
    "rnn = CharacterRNN(num_classes=len(chars), batch_size=batch_size)\n",
    "\n",
    "# Training our RNN\n",
    "rnn.train(train_x, train_y, \n",
    "          num_epochs=20,\n",
    "          ckpt_dir='./model-20/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <<< lstm_outputs >>> Tensor(\"rnn/transpose_1:0\", shape=(1, 1, 128), dtype=float32)\n",
      "Tensor(\"probabilities:0\", shape=(1, 65), dtype=float32)\n",
      "INFO:tensorflow:Restoring parameters from ./model-20/language_modelling.ckpt\n",
      "The as shand the win the heer of it oue merar the mess and ther and,\n",
      "Hat sond wart sing and ther and tis so the sis thas iter ther ind, bertis it and,\n",
      "Har as it and sathe hearthen to my were me thart, an tourd thet\n",
      "  am. I the he that tine te sould in shis ofrert ite mit or toue sinte thore\n",
      "\n",
      "\n",
      "   Hol. Hat. wil  aur thandes the wise herde the terie thangeris to d ion the mere se ang meser indt on this ande hor too hame the terine mat tis at in aster it arer tore mang so mind at thit shoue soree tere so\n"
     ]
    }
   ],
   "source": [
    "# We create a new instance of the CharacterRNN() class in the sampling mode\n",
    "# mode by specifying sampling=True\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "# Creating a RNN object with sampling mode\n",
    "rnn = CharacterRNN(len(chars), sampling=True)\n",
    "\n",
    "# Generate a sequence of 500 characters\n",
    "print(rnn.sample(ckpt_dir='./model-20/', output_length=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
